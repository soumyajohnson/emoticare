{"ast":null,"code":"var _jsxFileName = \"/Users/siddharthaghosh/Downloads/emoticare_voice_upgraded/client/src/VoiceInput.js\";\nimport { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\nconst VoiceInput = ({\n  onResponse\n}) => {\n  const handleClick = async () => {\n    const recognition = new window.webkitSpeechRecognition();\n    recognition.lang = 'en-US';\n    recognition.start();\n    recognition.onresult = async event => {\n      const transcript = event.results[0][0].transcript;\n      console.log(\"You said:\", transcript);\n      const res = await fetch('http://localhost:8000/analyze', {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json'\n        },\n        body: JSON.stringify({\n          text: transcript\n        })\n      });\n      const data = await res.json();\n      onResponse(data.response);\n    };\n    recognition.onerror = err => {\n      console.error(\"Speech recognition error:\", err);\n    };\n  };\n  return /*#__PURE__*/_jsxDEV(\"button\", {\n    onClick: handleClick,\n    children: \"\\uD83C\\uDFA4 Talk to EmotiCare\"\n  }, void 0, false, {\n    fileName: _jsxFileName,\n    lineNumber: 26,\n    columnNumber: 10\n  }, this);\n};\n\n// âœ… This line is required:\n_c = VoiceInput;\nexport default VoiceInput;\nvar _c;\n$RefreshReg$(_c, \"VoiceInput\");","map":{"version":3,"names":["VoiceInput","onResponse","handleClick","recognition","window","webkitSpeechRecognition","lang","start","onresult","event","transcript","results","console","log","res","fetch","method","headers","body","JSON","stringify","text","data","json","response","onerror","err","error","_jsxDEV","onClick","children","fileName","_jsxFileName","lineNumber","columnNumber","_c","$RefreshReg$"],"sources":["/Users/siddharthaghosh/Downloads/emoticare_voice_upgraded/client/src/VoiceInput.js"],"sourcesContent":["const VoiceInput = ({ onResponse }) => {\n  const handleClick = async () => {\n    const recognition = new window.webkitSpeechRecognition();\n    recognition.lang = 'en-US';\n    recognition.start();\n\n    recognition.onresult = async (event) => {\n      const transcript = event.results[0][0].transcript;\n      console.log(\"You said:\", transcript);\n\n      const res = await fetch('http://localhost:8000/analyze', {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify({ text: transcript })\n      });\n\n      const data = await res.json();\n      onResponse(data.response);\n    };\n\n    recognition.onerror = (err) => {\n      console.error(\"Speech recognition error:\", err);\n    };\n  };\n\n  return <button onClick={handleClick}>ðŸŽ¤ Talk to EmotiCare</button>;\n};\n\n// âœ… This line is required:\nexport default VoiceInput;\n"],"mappings":";;AAAA,MAAMA,UAAU,GAAGA,CAAC;EAAEC;AAAW,CAAC,KAAK;EACrC,MAAMC,WAAW,GAAG,MAAAA,CAAA,KAAY;IAC9B,MAAMC,WAAW,GAAG,IAAIC,MAAM,CAACC,uBAAuB,CAAC,CAAC;IACxDF,WAAW,CAACG,IAAI,GAAG,OAAO;IAC1BH,WAAW,CAACI,KAAK,CAAC,CAAC;IAEnBJ,WAAW,CAACK,QAAQ,GAAG,MAAOC,KAAK,IAAK;MACtC,MAAMC,UAAU,GAAGD,KAAK,CAACE,OAAO,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAACD,UAAU;MACjDE,OAAO,CAACC,GAAG,CAAC,WAAW,EAAEH,UAAU,CAAC;MAEpC,MAAMI,GAAG,GAAG,MAAMC,KAAK,CAAC,+BAA+B,EAAE;QACvDC,MAAM,EAAE,MAAM;QACdC,OAAO,EAAE;UAAE,cAAc,EAAE;QAAmB,CAAC;QAC/CC,IAAI,EAAEC,IAAI,CAACC,SAAS,CAAC;UAAEC,IAAI,EAAEX;QAAW,CAAC;MAC3C,CAAC,CAAC;MAEF,MAAMY,IAAI,GAAG,MAAMR,GAAG,CAACS,IAAI,CAAC,CAAC;MAC7BtB,UAAU,CAACqB,IAAI,CAACE,QAAQ,CAAC;IAC3B,CAAC;IAEDrB,WAAW,CAACsB,OAAO,GAAIC,GAAG,IAAK;MAC7Bd,OAAO,CAACe,KAAK,CAAC,2BAA2B,EAAED,GAAG,CAAC;IACjD,CAAC;EACH,CAAC;EAED,oBAAOE,OAAA;IAAQC,OAAO,EAAE3B,WAAY;IAAA4B,QAAA,EAAC;EAAoB;IAAAC,QAAA,EAAAC,YAAA;IAAAC,UAAA;IAAAC,YAAA;EAAA,OAAQ,CAAC;AACpE,CAAC;;AAED;AAAAC,EAAA,GA5BMnC,UAAU;AA6BhB,eAAeA,UAAU;AAAC,IAAAmC,EAAA;AAAAC,YAAA,CAAAD,EAAA","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}